\let\nofiles\relax
\documentclass[10pt]{article}

\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}

\usepackage[english]{babel}

% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% Leave date blank
\date{}

% Header and Footer with log
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{27.023pt}
\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\sf PLOS}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION
\usepackage{color}
%\newcommand{\noteCP}[1]{}
\newcommand{\noteMdK}[2]{(MdK: \textcolor{purple}{#1})}
\newcommand{\noteMP}[3]{(MP: \textcolor{blue}{#1})}
\newcommand{\noteCP}[1]{(CP: \textcolor{red}{#1})}
\newcommand{\notenewMdK}[2]{}
\newcommand{\notenewMP}[3]{(MP: \textcolor{blue}{#1})}

\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft} {\LARGE \textbf{Modelling the neural dynamics of
      binding in language with the Neural Blackboard Architecture} }
  % Insert Author names, affiliations and corresponding author email.
  \newline
  \\

  Martin Perez-Guevara\textsuperscript{1*}, Marc De Kamps\textsuperscript{2*}, Christophe Pallier\textsuperscript{1}
  \\
  \bigskip \textbf{1} Cognitive Neuroimaging Unit, CEA DSV/I2BM,
  INSERM, Université Paris-Sud, Université Paris-Saclay,
  Université Paris-Descartes, NeuroSpin center, 91191 Gif/Yvette, France
  \\
  \textbf{2} Institute for Artificial Intelligence and Biological
  Systems. School of Computing. University of Leeds. LS2 9JT Leeds.
  United Kingdom
  \\
  % Title must be 150 characters or less
  \bigskip
  % \institute{\textbf{1. }\\ \textbf{2. }\\}


(*) Corresponding authors.
  
\end{flushleft}

% Please keep the abstract between 250 and 300 words

\section*{Abstract}

In the course of sentence comprehension, the brain must assign words to their syntactic and semantic roles \cite{smolensky2006harmonic,Jackendoff_2002b}, relying on variable binding operations \cite{marcus14}.
One of the few attempts to model complete circuits capable of variable binding with spiking neural population networks is the Neural Blackboard Architecture (NBA) proposed by van~der~Velde and de~Kamps \cite{van_der_Velde_2006}.

Here we demonstrate that an NBA implementation, only tuned to operational constraints, naturally reproduces the neural activity patterns of two neuroimaging experiments involving linguistic binding at different spatio-temporal scales.

Nelson \emph{et al.} (2017) reveals high-temporal resolution neural dynamics of sentence comprehension from intracortical recordings (ECoG).
Here, we show how similar signals originate from a proposed architecture of neural circuits whose function is the neural implementation of the binding process, suggesting that Nelson \emph{et al.} indeed observed 
correlates of binding.
In an fMRI experiment, Pallier \emph{et al.} )(2011) found that the hemodynamic response of sentence processing showed a sublinear dependency
on the size of the syntactic constituents.
Our model also replicates this finding.

Our simulations are based on the dynamics of spiking point model neurons: leaky-integrate-and-fire (LIF) and adaptive-exponential-integrate-and-fire (AdEx) neurons.
Rather than simulating thousands of spiking neurons, we use population density techniques (PDTs) to model dynamics at the population level. Although related to rate based models, 
for PDTs the correspondence to population-averaged quantities of spiking neurons can be shown rigorously.
In particular transient dynamics are captured more accurately than by rate based models.

These results, alongside the flexibility of the NBA to represent arbitrary binary tree structures and parsing schemes, \notenewMdK{This is a strong statement that needs justification later in the paper.} \notenewMP{I hope this should be clear after reading the methods}
makes it a promising tool for linguistic hypothesis exploration and future refined quantitative accounts of multi-scale neuroimaging measurements.


\section{Introduction}

{\label{931947}}

Human languages rely on composition operations at several levels of representations: phonemes combine into morphemes which combine into words which themselves combine into 
phrases and sentences. For example, in Chomsky's Minimalist Program for Syntax \cite{Chomsky_2013}, the center stage is given to `Merge', the operation which combines words and phrases to create larger phrases.

Proposing a satisfactory neural model for the ``composition'' operations required by language is linked to solving the binding problem \cite{marcus14}.
The term binding was introduced into the neuro-scientific community by von der Malsburg\cite{von_der_Malsburg_1994} during the first explorations of neural phase synchronization for ``variable binding'', which is a term taken literally from computer science, meaning to link a data structure to a name so it can be later accessed by that name.
Binding is also motivated by the empirical discovery of the distributed and segmented encoding of features along the cortex. For example color and shape, in the case of vision, are robustly integrated during perception but can be independently impaired by brain damage.

The general binding problem actually comprises four different sub problems, namely~\emph{General Coordination},~\emph{Feature-Binding},~\emph{Variable Binding} and the~\emph{Subjective Unity of Perception}\cite{Feldman_2012}.
The current work is specifically motivated by the~\emph{Variable Binding} sub-problem that is presented by Feldman as an abstract high level cognitive faculty, mainly required by symbolic thought, to bind values to instances of variable types that form part of a structured representation.
In this respect, it is a function that arises mostly in human languages and goes beyond the extensively studied sensory, attention and short-term memory phenomena of feature binding, that only require linking features exclusively, in a bag, to avoid confusion with simultaneous representations, like a red circle and a blue square presented side by side in a screen.

~\emph{Variable binding} is illustrated by the capacity to run logical inference on data structures that encode relationships between their items.
For example the sentence ``\emph{Mary owns a book}" allows to establish a relation of the type \emph{own(Mary, book)} that implies \emph{owner(book, Mary)}, such that we can later ask the question ``\emph{Who owns this book?}'', which would not be answerable under a simpler feature binding mechanism that would just confuse the three words in a bag as just belonging to the same sentence.
To implement this in language, most linguistic theories propose that there are types of words, named grammatical categories, like 'noun' and 'verb', that are instantiated during sentence comprehension to be combined under a finite set of constraints.
These instantiated word types would point to each other to form a graph data structure, a tree, on which query and join operations can be performed, and they would also point to their corresponding specific words.
Then solving ~\emph{Variable binding} in language, requires a biologically feasible implementation of a pointer mechanism that can link instantiated grammatical categories and their corresponding words, which we will demonstrate with the Neural Blackboard Architecture.


\subsection{Some neuroimaging studies of binding of language}
{\label{intro:imaging}}

Most linguistic theories assume a constituency property that allows to combine and replace smaller phrases in larger phrases.
Since solving variable binding requires an explanation of how to implement links between bits of information - like words and word types - to create basic data structures, like phrases in language, it is likely to also explain how to create links between such basic structures.

Behavioral evidence for constituents in phrases has been around for a while\cite{bever1969underlying, abrams1969syntactic}, with more recent studies demonstrating the reuse of 
recently heard syntactic structures through syntactic priming experimental paradigms\cite{bock2007persistent, branigan2000syntactic}.
But only recently we have started to characterize the detailed neural correlates of constituency and word binding with diverse brain-imaging techniques
\cite{Nelson_2017, fedorenko2016neural, brennan2016abstract, ding2016cortical, bemis2012basic, Pallier_2011, bastiaansen2010syntactic, longe2006grammatical}.

We selected The ECoG analysis of Nelson et al.\cite{Nelson_2017} as the first study to compare to our model.
It is one of the only two studies so far demonstrating spatially specific and temporally detailed neural dynamics of phrase processing, 
made possible by analyses of intracranial neurophysiological data taken from epileptic patients.
Moreover it is the first one to characterize the specific patterns of phrase-structure formation, possibly revealing the first neural signatures of variable binding 
related operations. Nelson et al. refer to them as "merge" operations that combine syntactic objects (word types and phrase types).
In the study words were presented sequentially to patients in a screen to be read under a Rapid Serial Visual Presentation paradigm.
The task was to keep a phrase of up to 10 words in memory to compare it just after with a probe sentence composed of 2 to 5 words.
We will show that simulation of the NBA portion responsible for variable binding, while only tuned for correct operation, generates strikingly similar temporal patterns of 
neural activity when aggregating the binding operations corresponding to complete phrase processing, assuming the phrase grammar and bottom-up parsing scheme employed by 
Nelson et al. in their analyses.

As a second study, we selected an fMRI experiment \cite{Pallier_2011} to portray the capacity of the model to capture results from multiple neuroimaging spatio-temporal scales.
In this experiment, trials with lists of 12 words obtained by concatenating phrases of a given length, were presented to healthy subjects.
Conditions were formed from all combinations of m by n that give 12, satisfying the form n phrases of m words, like 2 phrases of 6 words.
Besides normal words, the design also included pseudoword conditions that maintained morphological markers and closed-class (function) words.
This allowed the authors to demonstrate a clear separation of syntactic and semantic binding neural activation patterns in language related regions, which is interesting to us, 
since syntactic specific patterns are the closest to the abstract considerations of binding of our model, assuming the same phrase grammar and parsing scheme employed for 
comparison with the ECoG results.
The authors found a sub-linear pattern of neural activation as the number of constituents increase, which could not be explained by a simple "accumulation" model motivated by 
measurements of sequence learning tasks in awake macaque monkeys.
The Neural Blackboard Architecture predicts this sub-linear effect from the circuit recruitment process required by the number of binding operations, 
alongside expected patterns of hemodynamic peak onset differences from delay activity considerations.


\subsection{Neural models of language}

{\label{619233}}

To understand how the cognitive faculty of language operates, we need to take into account, not only the underlying supporting structures, but also their dynamics. 
This means that we have to consider simultaneously the grammars given by linguistic theory and a temporal component to give birth to computational mechanisms, like 
automaton models, capable of explaining behavior\cite{hale2014automaton}.
To extend this into neuroscience we have to go even further and also provide reasonable implementation models, corresponding to the biological components of the brain.
This implementation is necessary to be able to go beyond behavioral measurements and ultimately test computational hypotheses directly against the currently available spatio-temporal neural measurements.

A good example of success in this direction is the computational~theory of visual receptive fields\cite{lindeberg2017normative} which has made impressively accurate predictions 
about the shape of the biological visual fields in the retina.
Knowledge of these basic units of visual perception has even recently allowed to correlate the mechanisms behind deep convolutional neural networks to visual 
pathways\cite{Guclu_2015,Eickenberg_2017} and has influenced our understanding of higher-level visual phenomena such as visual illusions\cite{Eagleman_2001}.
Although expecting at the moment something similar in the case of language might sound overambitious, we must note that basic phonetic features have already been decoded in the 
Superior Temporal Gyrus from electrocorticography (ECoG)\cite{Mesgarani_2014}.
Moreover, as presented in the previous section, recently several spatial and temporal detailed neural correlates of constituency and binding have been revealed.

Numerous Artificial Neural Networks (ANNs) have been implemented, motivated by biological principles in the brain, to model particular aspects of brain language function or to reproduce behavior in specific language tasks.
Nonetheless they lack dynamic biological considerations necessary to match their output with neuroimaging measurements, and except for Vector Symbol Architectures (VSA)\cite{smolensky2006harmonic}, they are difficult to integrate into a general framework for the implementation of the complete language function.

For an extensive review of these models, categorized by language function, we recommend reading Bocancia\cite{bocancia2014psycholinguistically}, 
Christiansen et al.\cite{Christiansen_1999} and Miikkulainen\cite{miikkulainen1997natural}.
Among these we mention the Shruti architecture for logical inference\cite{Wendelken_2004} and the latest optimization and quantization framework of Smolensky for phonological 
production\cite{Smolensky_2013}.

More relevant to our work are previous efforts to model language function with more biologically plausible Spiking Neural Networks (SNNs), 
that would eventually allow to establish a mechanistic link between neural measurements and computational linguistic hypothesis.
Contrary to the VSA and the Neural Blackboard Architecture (NBA)\cite{van_der_Velde_2006}, these do not follow a general theoretical framework, to address all the neural challenges of a complete language function implementation, that can also provide a mechanistic explanation for the most basic computational components and behaviors.

Representative examples of these models are CABot\cite{Huyck_2009}, BioPLa\cite{rosa2004thematic}, TRANNS\cite{bocancia2014psycholinguistically}, the language models of Markert et al.\cite{Markert_2007}, the corticostriatal model of Dominey et al.\cite{Dominey_2009}, the discrete combinatorial neural assemblies of Pulvermuller\cite{Pulverm_ller_2009, Pulverm_ller_2010}, the words model of Garagnani et al.\cite{Garagnani_2017} and the DORA mechanism proposed by Martin et al. for hierarchical linguistic structures\cite{Martin_2017}.

In most models, biological details necessary to match high temporal resolution in-vivo neural patterns of language processes have been kept out of scope.
This has been a reasonable strategy considering the computational cost of building circuits with detailed neural models based on point processes.
Yet, as we will show, recent developments like population density
techniques\cite{de2013generica} now permit to simulate state-of-the-art temporally detailed dynamics of circuits of neural populations.

Instead of trying to implement a complete language parsing neural network trained from stimuli, we will focus on pushing the boundary of biological detail for the specific, and most basic, abstract mechanism of variable binding as proposed in the Neural Blackboard Architecture\cite{van_der_Velde_2006}, assuming other necessary mechanisms for parsing are in place.
This simple mechanism will already provide temporally detailed predictions about the neural signatures of variable binding, to be contrasted with the brain imaging results obtained from ECoG and fMRI experiments.
Moreover it can act as the center piece for future integration of the rest of the NBA mechanisms covering other functional aspects of the brain language function.


\subsection{Main proposals for variable binding implementation}

{\label{946708}}

Due to the combinatoric nature of language structures, modelling binding with simple linking mechanisms like combination-encoding cells 
become unfeasible\cite{von_der_Malsburg_1999}.Van der Velde\cite{van_der_Velde_2006} cites a simple example: an average 17-year-old English speaker has a lexicon of more than 
60.000 words, which easily allows a set of 10\textsuperscript{20} or more possible phrases of 20 or less words, that can be produced and understood.
Such a magnitude effectively exceeds the estimated lifetime of the universe expressed in seconds, 
making it clear that a simple holistic encoding of sentences is not attainable in a human lifetime.
Moreover the need to perform later unbinding operations for information extraction in linguistic data structures where order and variable types matter 
adds constraints to the possible mechanisms we can hypothesize.
To surmount these issues, two main approaches are discussed in the literature.
The first relies on the framework of tensor product representations\cite{smolensky2006harmonic} and 
the second on connectivity based models that allow ``binding by process''\cite{van_der_Velde_2015}.

Smolensky provides an in-depth analysis of the typology of vector symbol architectures (VSA) that work with tensor operations.
He shows that various proposals, including Synchronous Firing\cite{Shastri_1993}, Holographic Reduced Representations\cite{Plate_1995} and Recursive Auto-Associative Memories\cite{Chalmers_1992}, are just considered to be particular cases with varying implementation details of his general tensor framework.

He proposes that the brain employs explicit active encodings, in neural units, of ``unified'' data structures produced by tensor products acting as binding operations, 
that can be later queried with tensor contractions acting as unbinding operations.
The latter are resilient to squashing functions, like those proposed by Plate, that can importantly decrease the number of neural units necessary for the final 
representation as the tensors increase in dimensionality with more complex structures.
Smolensky offers in great detail implementations of VSA with feedforward and symmetric recursive ANNs\cite{smolensky2006harmonic} and has recently shown 
how to extend the framework with an optimization scheme to instantiate input representational vectors\cite{Smolensky_2013}.
Nonetheless, no important operational consideration is given to time, although it is possible to employ it as a tensor for vector encoding purposes, as is done for Synchronous Firing.
This limits the neural dynamics predictions of the framework and its interpretation with SNNs.

On the other hand, Van der Velde and De Kamps \cite{van_der_Velde_2015} argue in favor of a small world network model that, thanks to its connectivity properties 
allows the formation of complex structures.
It does so by conditionally coactivating neural assemblies representing grounded concepts and instances of variable types, which is a process driven by a control mechanism.
In this framework, working memory acts as a control that reduces inhibition on paths of neural flow necessary to maintain the bindings 
established by the initial transient coactivation, such that pointers have been  declared implicitly between the coactivated concepts.
Contrary to the tensor framework, data structures are implicitly encoded by the short lived reinforced paths of neural activity flow.
Then query operations are possible by reactivating nodes - included in the query - that induce coactivation of answer nodes, thanks to the reinforced connectivity.
This successive coactivation of neural assemblies, that leads to a short-term lived graph that implicitly encodes the final data structure, 
is referred to as ``binding by process''.
Van der Velde and De Kamps insist on the need of connectivity considerations to produce behavior in a sensory-motor loop and to provide an internal frame of reference for the brain to implement queries.

The latter idea is at the basis of the Neural Blackboard Architecture (NBA) proposed by Van der Velde and De Kamps \cite{van_der_Velde_2006}. Thanks to its properties, 
it can solve the challenges posed by Jackendoff to model language in the human brain, namely ``The massiveness of binding'', ``The problem of 2'', ``The problem of variables'' and ``Binding in working memory vs long-term memory''.
First ``The massiveness of binding'' is addressed by instantiation of variable types as assemblies that are binded to grounded concepts and other variable types instances, 
allowing the creation of combinatoric structures on demand.
Then ``The problem of variables'' is handled by the previously explained coactivation mechanism capable of creating pointers from grounded concepts to variable type instances.
``The Problem of 2'' is managed by having multiple neural assemblies that instantiate the same variable type in the architecture but that can occupy different parts of the same data structure.
Finally a working memory mechanism is provided, as explained in the previous paragraph, that allows transient short-term coactivations of concepts to be maintained without interfering with the possibility of storing related data structures in the long term in other parts of cortex with other mechanisms.

The level of abstraction of the NBA allows to apply it to several cognitive functions like motor control, attention and symbolic thought.
In the case of syntactic parsing during language comprehension, one needs a grammar to specify the necessary variable type relations and some parsing scheme to determine the bindings' timing.
In contrast to VSA, the NBA provides a circuit with nodes that can be readily interpreted in terms of spiking neural populations.
This can be conceptually linked to the notion of cell assemblies, whose existence and functional relevance, as computational units, is supported on substantial biological evidence\cite{Huyck_2013}.


\subsection{The Neural Blackboard Architecture (NBA) applied to language}

{\label{935508}}

There are several previous instantiations of sub-circuits of the NBA with varying degrees of biological plausibility, 
the latest relying mostly on Wilson Cowan population dynamics\cite{Destexhe_2009}.
Some of the previous simulations attempted to address diverse aspects of language processing, 
such as ambiguity\cite{Frank_2014} and learning control from syntactic stimuli\cite{van_der_Velde_2010}.
Other simulations addressed circuit implementation issues like how to develop a connectivity matrix with randomly 
connected networks\cite{van_der_Velde_2011} and how to implement a central pattern generator sub-circuit for sequential activation~\cite{van_Dijk_2015}
In the following paragraphs we summarize the main abstract mechanisms and assumptions behind the NBA. A 
complete illustration of the blackboard architecture is provided in Figure {\ref{Blackboard}}.
For a deeper review we recommend reading a recent paper with a circuit design and examples that focus on sentence processing\cite{de2016combinatorial}, as well as the 
original framework proposal introducing abstract combinatorial structures\cite{van_der_Velde_2006}.


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.00\columnwidth]{figures/gating_circuit3}
    \caption{The Neural Blackboard architecture.
      \textbf{A.} Gating circuit that allows the implementation of conditional neural activity transfer between Neural assemblies X and Y through a gate assembly.
      The gate keeper assembly (GK) is activated by the X assembly and then inhibits the gate assembly (G).
      To let information flow through the gate assembly, a control assembly (Ctl) must therefore inhibit the gate keeper assembly.
      \textbf{B.} Architecture of a single compartment circuit of a connection matrix.
      Six gating circuits are arranged in a way that makes conditional bidirectional neural activity flow between two main assemblies possible.
      Control assemblies regulate the direction of information flow and allow the activation of sub assemblies.
      The two sub assemblies excite the working memory assembly which, once activated, encode the binding of the main assemblies and allow activation to flow between them if the controls allow it too.
      \textbf{C.} Each connection matrix contain n by m compartment circuits that encode the same relationship type between the same pair of assembly categories.
      There are $m$ available assemblies for one category and $n$ available assemblies for the complementary category and only one cell circuit can activate its working memory assembly to link two particular assemblies due to mutual row and column inhibition of cells in the connection matrix.
      The size of the connection matrix effectively represents memory limitations.
      A blackboard is composed of an arbitrary number of connection matrices that encode different relationship types for a pair of assembly categories.
      \textbf{D.} A blackboard is composed of multiple connection matrices, where each of them is defined by two node categories and a relationship type between them.
      \textbf{E.} Example of a possible tree structure that can be represented based on the specified connection matrices. }
      \label{Blackboard}
  \end{center}
\end{figure}


Nodes in Figures {\ref{Blackboard}}.A and {\ref{Blackboard}}.B represent neural assemblies that can be interpreted as linked spiking neural populations.
The most basic component of the NBA is a ``Gating Circuit'' illustrated in Figure {\ref{Blackboard}}.A.
The main idea is that neural activity would flow from the assembly X to the assembly Y, but is blockedby the Gake Keeper (GK) assembly, 
which itself is excited by assembly X.
So to allow directional activity flow from X to Y, a Control (Ctl) assembly has to inhibit the GK assembly.
Notice that it is trivial to extend the gating circuit for bidirectional control of activity flow as illustrated in Figure {\ref{Blackboard}}.B.
Introducing bidirectional conditional control signals is what gives the NBA the possibility of implementing separately queries like 'what follows X?' or 'what follows Y?'.

The second basic component of the NBA is a proposal for working memory (WM).
Persistent neural activity in response to stimuli is considered to be the neural process underlying active (working) memory, and its implementation is hypothesized to be based on excitatory reverberation\cite{wang2001synaptic}.
Based on this, the NBA considers a Delay Activity\cite{de_Kamps_2005} mechanism as a biologically plausible implementation of WM. It consists on a neural assembly, that after being excited beyond a certain threshold, achieved by the coactivation of input populations, will maintain a constant amount of activation for a short period of time. By maintaining its activity, WM acts as a short lived bidirectional link between two assemblies. This mechanism can be considered as the creation of an implicit pointer from one assembly to the other, such that future reactivation of one assembly can be driven from the other to perform query operations. This conforms a ``Memory Circuit'' as depicted in Figure {\ref{Blackboard}}.B.

Two bidirectional ``Gating Circuits'' connected by a ``Memory Circuit'' form a ``Compartment Circuit'' capable of implementing variable binding and query operations.
The key point of this circuit is that Main assemblies (MA), representing grounded concepts or instances of variables types, activate Sub assemblies (SA) 
if a control signal driven by another mechanism allows it.
Then coactivation of SAs is what realizes a temporary binding of MAs by activating WM.
So one ``Compartment Circuit'' models specifically the neural activity of a variable binding operation.
It is operated by a mechanism that drives control signals simultaneously in multiple ``Compartment Circuits'' to instantiate binary tree like data structures on which query/unbinding operations can be performed later. 

As might be evident by now, applying the NBA to syntactic processing in language consists of two simple assumptions.
First, equating the parsing mechanism to the control mechanism that coordinate binding events of words and word types and phrase types.
Second, determining the number of compartment circuits necessary to instantiate a complete syntactic structure and the content of MA nodes from a grammar theory.
In this work we will only employ a phrase grammar and bottom-up parsing scheme following theoretical assumptions of selected neuroimaging experiments.
Nonetheless, a promising feature of the NBA is that it has the flexibility to test any arbitrary parsing mechanism incorporating top-down considerations and an important variety of alternative theories of grammar based on binary trees.
For example dependency grammars that assume multiple direct word bindings instead of the hierarchical phrase bindings modelled in this work have been employed in previous simulations\cite{van_der_Velde_2010}.

To understand how a sentence is processed in the NBA, let us consider first the simplest case of binding two words, like ``Sad student'', 
belonging to grammatical categories instantiated in the MAs of one ``Compartment Circuit'', such that one MA is an ``Adjective'' corresponding to ``sad'' and the other one 
is a ``Noun'' corresponding to ``student''.
The MAs activate with timings corresponding to word presentation, so we are assuming that words were recognized to motivate their corresponding instantiated grammatical 
categories before we attempt to link them.
Then an assumed parsing mechanism determines that a link operating on ``Adjective'' and ``Noun'' types is necessary in the blackboard, driving activity in several 
``Compartment Circuits'' from which only one, that we consider as the recruited ``Comparment Circuit'', completes coactivation of SAs to drive WM and realize binding between the word types.

In the case of a complete phrase, like ``Fat sad student'', if we are assuming the instantiation of phrase types that form a hierarchical tree theorized by a phrase grammar, 
then the time at which the binding of the instantiated grammatical categories of ``sad student'' takes place would be the time at which a ``Noun Phrase'' is activated and bound 
to the ``Adjective'' corresponding to ``Ten''.

Finally, a ``Connection Matrix'', portrayed in Figure {\ref{Blackboard}}.C, allows the implementation of a complete ``Blackboard''.
It contains variable type relations learned by the ``Blackboard'' as sets of mutually inhibitory ``Compartment Circuits'' that enable the selection of the 
``Compartment Circuits'' requested by the control mechanism.
We portray the ``Blackboard'' as a regular grid for illustrative purposes, although there is already a proof of concept implementation with 
randomly connected networks\cite{van_der_Velde_2011}.
Also implementing a general syntactic control mechanism should be feasible. As suggested by the Feed-forward artificial 
neural networks employed in previous NBA simulations~\cite{van_der_Velde_2010} and recent state of the art feedforward network architectures that have shown top performance for diverse language parsing tasks~\cite{andor2016globally}.
Moreover a more recent proposed extension of the NBA, that imitates the motor circuit of the marine mollusk Tritonia diomedea, 
shows how to generate patterns for sequential activation control\cite{van_Dijk_2015}.
Simulating these higher level mechanisms is a task out of the scope of this work, since we focus specifically on reproducing the neural signatures of variable binding operations.


\section{Methods}

{\label{488128}}

\subsection{Simulation framework}\label{simulation-framework}

We assume that the NBA lives in the cortex, and seek a good compromise between realistic modelling of the cortical dynamics and the tractability of the simulation.
State-of-the-art simulations of larger cortical structures are based on point model neurons that allow the inclusion of biological details such as synaptic dynamics and adaptation, but are restricted to about the size of a cortical column \cite{potjans2012cell}.
For larger scale networks, such as ours, a population-based approach is currently the only feasible approach.
The two choices are: rate based models or population density techniques (PDTs).
In rate based models, the population is described by a single variable, usually related to the population firing rate or average membrane potential of neurons in the population. 
A prominent example is the  Wilson-Cowan equation \cite{wilson1972excitatory}, which describes the dynamics of the population activity as a first order linear differential equation driven by inputs.
Another example is the Jansen-Rit model \cite{jansen1995electroencephalogram}, which is primarily motivated by phenomenological considerations.
In both examples, the relationship with the underlying neural state is unclear. We have opted for PDTs, also a population based approach, but one where the relationship with the dynamics of a group of spiking point model neurons can be made rigorous.
Although they are computationally more expensive than rate based models, they are easier to manage than a full-blown model using spiking neurons, which would need hundreds of thousands of neurons at the scale of the cortical network considered here.
We will briefly set out the assumptions that we use in modelling populations and describe the numerical methods involved.

Consider a leaky-integrate-and-fire (LIF) neuron, which is characterized by a single state variable: the membrane potential.
If the neuron has a potential different from its equilibrium potential, or when it experiences an external drive, for example generated by a synaptic current, the potential evolves according to:

\begin{equation}
\tau \frac{dV}{dt} = -(V - V_{rev}) + I(t).
\label{eq-lif}
\end{equation}

Here $V$ is the membrane potential in V, $\tau$ the membrane time constant in s, $V_{rev}$ the reversal potential and $I(t)$ and external current, which may comprise contributions from other neurons in the form of spikes, and therefore may be stochastic.
If the membrane is driven far above the equilibrium potential, at a potential $V_{th}$, the threshold, the neuron spikes.
We assume it will be inactive for an absolute refractive period $\tau_{ref}$ and then finds itself reset to the equilibrium potential after that.  
This scenario is easy to simulate: using a simulator like NEST \cite{gewaltig2007nest}, or BRIAN \cite{stimberg2014equation}, one can create populations of LIF neurons.
In the simplest case a population is driven by synthetically generated input spike trains, where the spike train events are created by a random generators.
The default assumption is that inter-spike intervals are Poisson distributed, although this can be extended to non-Markov processes \cite{lai2017population}.
It is clear that $I(t)$ in Eq. \ref{eq-lif} now should be considered as a stochastic variable and that the threshold crossings of LIF neurons themselves are stochastic events as a consequence.
Fig. \ref{fig:pdt-case} A  demonstrates a simple scenario: a population of 10000 LIF neurons, driven by a stochastic input - Poisson generated spike trains, where each LIF neuron experiences about 800 input spikes per second.
The simulation shows a spike raster of the population response:
first nothing: although each LIF neuron receives input spikes and as a consequence has its membrane potential driven up, none of the neurons have reached threshold;
then a spike volley: most neurons hit threshold at approximately the same time;
followed by a period of relative silence: only interrupted by a few stragglers;
at last a gradually achieved final neural state of asynchronous random firing.
More complex networks can be formed by feeding the output spikes of one population into other populations.

This is a fascinating but unwieldy process and statistical methods have been used to describe it at the population level \cite{stein1967some,knight1972dynamics,omurtag2000simulation}.
A population is described by a density function, which expresses how the population is distributed over state space.
For LIF neurons this is a function $\rho(V)$, where $\rho(V)dV$ is the fraction of neurons with their membrane potential in interval $[V, V + dV)$ (when we integrate the density function over a certain state interval, we will refer to the  result as  the amount of \emph{mass} in that interval).
The initial distribution of the neurons in the population must be chosen, but the evolution of the density is tractable.
It is clear that neurons move through state space due to the deterministic neural dynamics, Eq \ref{eq-lif} for LIF neurons, and also go transitions due to the input spikes.
The collective contribution of the stochastic process to the evolution of the density profile can be  modelled using a Poisson master equation \cite{crispin1994handbook}; the contribution of the deterministic dynamics  can be modelled using an advection equation (see \cite{omurtag2000simulation} for a lucid explanation). 


As a consequence, the process of simulating thousands of neurons is now replaced by modelling the evolution of a density which is given by a single equation:

\begin{equation}
\frac{\partial \rho}{\partial t} -\frac{1}{\tau}\frac{\partial}{\partial v}(\rho v) = \int dh p(h) \nu (\rho(v - h) -\rho(v)),
\label{eq-synapse}
\end{equation} 

Here $p(h)$ is the distribution of synaptic efficacies, $\nu$ the frequency of the incoming spike trains, $\rho$ the density function, $t$ the time since start of simulation and $v$ the membrane potential.
Mass that is being pushed across threshold corresponds to neurons spiking; consequently  the firing rate of the population can be calculated directly from the mass flux across threshold.

Efficient and stable simulation methods are available \cite{nykamp2000population, de2003simple, de2013generic, iyer2013influence}, and remarkably, the process of solving Eq. \ref{eq-synapse} is computationally less expensive for LIF neurons than the direct simulation using NEST \cite{nykamp2000population}.
The process of keeping track of a single density function, and the communication between populations using firing rates rather than individual spikes, frees the modeller from keeping track of thousands of spikes per second and leads to simpler simulations.
Figure \ref{fig:pdt-case} shows the very close correspondence between direct simulations of LIF spiking neurons and population density results.
It shows, first, that the simulation results indeed are very close to that of the spiking simulation, and second, that Wilson-Cowan dynamics must be tuned in a way that PDTs do not: the correct steady state activation must be provided to the Wilson-Cowan dynamics in the form of a sigmoid, while in PDTs the correct steady state firing rate is calculated from first principles - input firing rate, synaptic efficacies and neural parameters - without any need for tuning. 


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.0\columnwidth]{figures/pdt_overview.pdf}

    \caption{\textbf{A.} A spike raster showing an LIF population undergoing a jump response.
        Neurons are at equilbrium at $t = 0$. From $t=0$ each neuron receives a Poisson distributed input spike train ($\lambda$ = 800 Hz, $h= 0.03$, i.e. an input spike raises the PSP by 3\% of the difference between threshold and equilibrium potential, $\tau = 50$ ms, following \cite{omurtag2000simulation}).
        \textbf{B.} Firing rate calculated from the PDT method (solid curve), compared to firing rate from spiking neuron simulation (red markers).
        \textbf{C.} The density calculated by the PDT method (solid curve) at $t=0.3$ s, compared to a histogram of the membrane potential over the population at the
        same time.
        \textbf{D.} Wilson-Cowan prediction for the firing rate, compared to PDT result. Importantly, Wilson-Cowan output must be tuned: the steady state value to which it converges is not predicted by the Wilson-Cowan equations, but must be provided as a sigmoid. 
       In contrast, the PDT method calculates the firing rate from first priciples, and agrees well with the spiking neuron simulation, within statistics.
      }
\label{fig:pdt-case}
  \end{center}
\end{figure}


The population density formalism can be extended to higher dimensional models.
For example, the adaptive-exponential-integrate-and-fire neuron (AdEx) \cite{brette2005adaptive} is a two dimensional model that has the membrane potential and an adaptivity parameter as a variable.
Consequently, the state space is two dimensional.
The motivation behind this model is that first, it includes adaptation, and second that it is the effective approximation of the complex conductance-based processes that take place in a real neuron.
The equations of the model are:

We consider the AdEx model as presented by Brette and Gerstner \cite{brette2005adaptive}, which describes individual neurons by the following equations:
\begin{align}
  C_m \frac{dV}{dt}    & =  -g_l(V - E_l)  + g_l e^{ \frac{(V - V_T)}{\Delta_{T}}} \\
  \tau_w \frac{dw}{dt} & =  a(V-E_l) -w \nonumber
\end{align}

Where $C_m$ is the membrane capacitance, $g_l$ the leak conductance, $E_l$ the leak potential (equivalent to the reversal potential for the LIF), $V_T$ a threshold potential, $\Delta_T$ a shape parameter for the spike, $\tau_w$ the adaptation time constant, $a$ the subthreshold adaptation parameter,  $V$ the membrane potential and $w$ the adaptation parameter. Upon a spike, the neuron is undergoes a transition in $w$: $w \rightarrow w +b$, where $b$ is the spike adaptation parameter.
We use the parameters given by Brette and Gerstner (2005).


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.0\columnwidth]{figures/aexp_overview.pdf}
    \caption{{AdEx dynamics {\label{fig-adex}} Left: Overview of AdEx dynamics.
        Right: a heat plot of the density profile during simulation. On the horizontal axis the membrane potential, on the vertical axis the adaptivity parameter. Note that
 the right figure constitutes a considerable reduction of state space compared to left. For the connectivity parameters we use, the state space on the right is the
part of state space reachable by dynamics.
      }}
  \end{center}
\end{figure}

We illustrate the dynamics of the neuron in Fig. \ref{fig-adex}.
The direction of the dynamics is shown by arrows, the speed of the dynamics by the size of the cells:
big cells implies fast dynamics as the cells represent equidistant time steps.
This shows that at $w =0$ dynamics are leaky,  i.e. towards the equilibrium, except at high values of $V$, on the right, which corresponds to spike generation.
At high values of $w$, there are two effects: stronger leak (larger cells) and a lower (more negative) equilibrium potential, which makes it harder for a cell at high $w$ to be driven across threshold, precisely the effect one expects due to adaptation.
At low $w$, the opposite happens: cells become more excitable.
For very low $w$ values, which can not be reached under cortical conditions, at least not for the parameters we used, there is the theoretical probability of a rebound (neuron always spikes).

A density function now lives in this two dimensional space: $\rho(V,w)$.
The evolution equation is a direct generalization of Eq. \ref{eq-synapse}.
For a model with $n$ state variables $\vec{V}$, a point model takes the form:
\begin{equation}
\tau \frac{d \vec{V}}{dt} = \vec{F}(\vec{V})
\end{equation}
and the density equation:
\begin{equation}
\frac{\partial \rho}{\partial t} + \frac{\partial}{\partial \vec{V}} \cdot \frac{( \vec{F} \rho)}{\tau} = \int dh p(h) \nu (\rho(\vec{V} - \vec{h}) -\rho(\vec{V}))
\end{equation},
where $\vec{h}$ represents the effect of an input spike.

We represent the density function by a heat plot on state space: the highest values or white, low values are red.
We are able to simulate the density function by a method analogous to that of \cite{de2013generica,iyer2013influence}, generalized to two dimensions.
In Fig. \ref{fig-adex} we show the result of a simulation: the density function as a fixed point in time.
As before, we can calculate the firing rate of the population by calculating the the flux across threshold (which is still given by $V= V_{threshold}$, i.e. the right hand side of the grid).

The simulation software, MIIND, is publicly available \url{http://miind.sf.net}. The LIF version of the algorithm has been available for some time \cite{de_Kamps_2008}, while 
the two dimensional version has become available recently \cite{dekamps2017b}.


\subsection{NBA simulation}\label{architectural-decisions}

Previous simulations of the NBA approximate the mean activity of neural assemblies with Wilson Cowan dynamics \cite{Frank_2014}.
Nonetheless, as explained in section \ref{simulation-framework}, direct simulations of leaky-integrate-and-fire (LIF) neurons \cite{omurtag2000simulation} have different transient behaviour than the dynamics described by the Wilson Cowan equations.Since we are interested in modelling the transient dynamics of variable binding in order to compare 
the simulation with real temporally detailed patterns of intracortical neural measurements like ECoG, we feel the need to model spiking neuron dynamics is important.

The decision to use AdEx, rather than LIF neurons has two motivations: first, adaptation is ubiquitous and its inclusion has a substantial impact on the dynamical range
allowed within the constraints of the blackboard architecture. Second,  it has been shown that 2D models, like AdEx, can already predict correctly 96\% of the spikes of 
detailed conductance models\cite{brette2005adaptive}.  Also, this model reproduces many known electrophysiological features, as can be appreciated in the spike-frequency adaptation review of Benda et al. \cite{Benda_2003,Benda_2014}. Our approach is consistent with a trend towards simpler, geometrically motivated  2D   models  that preserve the essence of more complex biophysically motivated models \cite{izhikevich2007dynamical}.

AdEx is now available in  MIIND. To our knowledge this is the first time that the AdEx model will be employed to approximate the neural dynamics of a circuit of this magnitude reproducing cognitive function.

In the case of Delay Activity (DA) populations like Working Memory (WM), we decided as a first approach to model such a mechanism phenomenologically.
We plan to address the different alternatives to model persistent cortical activity with interacting neural populations in future work.
As suggested by de Kamps\cite{de_Kamps_2005} not only models of recurrent excitation but also recurrent inhibition can account for this phenomena.
In the current simulation, a constant firing rate for DAs is kicked off by a specified level of input, resulting in activation  that is sustained for a predetermined period of time.
Contrary to previous simulations \cite{velde2015ambiguity}, we do not consider Sub-Assemblies (SAs) as DA populations.
We find that SAs can show rich and interesting dynamics just by fulfilling their function of mediating activation for WM.

We model Main-Assemblies (MAs)  as receiving input from DA populations, representing word types in some cases, and WM populations representing phrase types in other cases.
We do this to satisfy the assumptions of a phrase grammar that requires representation of deep tree hierarchical structures, so that we can separate the notion of a phrase resulting from previous word type bindings stored in WM, from the recruitment of MAs representing word grammatical category instantiations that take place during sentence processing.
Note that for other grammar types, like dependency grammars considered in previous NBA simulations\cite{velde2015ambiguity}, to consider words as nodes in their syntactic representations, we would only need to model word types for the MAs of the necessary compartment circuits.


\subsection{Compartment circuit parameters}\label{sec:circuit-parameters}

The compartment circuit contains two different types of neural populations.
Artificial neural populations following a boxcar event model, shown in Figure \ref{fig:circuit-spec}.B and biological neural populations following LIF or AdEx neural models.
We took LIF parameters from Omurtag et al. (2000) \cite{omurtag2000simulation} and AdEx parameters
from Brette and Gertsner \cite{Brette_2005}.

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.00\columnwidth]{figures/circuit_specs3}
    \caption{{Compartment circuit example {\label{fig:circuit-spec}} A.
Details
        of the Compartment Circuit implementation.
Only half of the
        circuit is shown since the design is symmetric.
The baseline
        (B) and Event input (Inp) populations are part of the
        simulation and not of the original abstract circuit proposal.
        B.
The behavior of the artificial neural populations and their
        selected parameters is shown%
      }}
  \end{center}
\end{figure}

As a first step we wanted to only explore the general behavior of the circuit of neural populations following well studied sets of parameters.
Nonetheless it is clear that studying the neural dynamics of specific brain regions might require adapting the parameters of the neural models to local measurements.
Each neural population is either excitatory or inhibitory; this means that a population that is excitatory (inhibitory) on one population is excitatory (inhibitory) on others as well, respecting Dale's law.

The dynamics of most populations are given by the PDTs and ultimately determined by the underlying model of spiking neurons.
These neural populations comprise a pair of Main Assemblies (MA), a pair of Sub Assemblies (SA), six Gate Assemblies (G) and six Gate Keeper Assemblies (GK).

Nonetheless there are a few other populations for which we simplified the simulation to the phenomenological level with an imitation of Delay Activity, which means that, after transient stimulation, a population retains its activation above a certain threshold for a given period of time.
For instance, the biophysical mechanisms of WM are still not understood completely, but its characterization as Delay Activity is relatively uncontroversial.
We modelled in this way, Control assemblies (Ctl), Working memory assemblies (WM), Event Input Assemblies (Inp) and a Baseline Assembly (B) that drives baseline neural activity of all completely simulated neural populations.
A complete diagram of the compartment circuit with example parameter values for LIF populations is given in Figure \ref{fig:circuit-spec}.\\~\\

We use a boxcar event model for persistent activity.
This model requires specification of the starting point of events, the persistent firing rate of the population and the duration of the persistent activity.
In the case of the Delay Activity of WM we also have to provide a kickoff input rate threshold that automatically triggers the boxcar event instead of providing a start time point.
The duration of persistent activity was pragmatically set up long enough for the neural dynamics to reach steady state and allow the formation of  all required bindings between phrase types and 
word types.
Finally the persistent activity rate and kickoff rate threshold were arbitrarily selected from possible parameter range values as a result of simulations of the circuit dynamics that will become clear in the following section.

Selecting firing rates to tune the compartment circuit is a complex task given the contrast between the extremely simplified circuit and real neural networks that contain multiple types of neurons with diverging behavior across cortical layers \cite{Wohrer_2013}.
Wohrer et al \cite{Wohrer_2013} show, from measurements in rat cortex, that the actual firing rate distributions of neural networks do not differ much between resting state and evoked activity.
The small difference would come from very few neurons that manage to drive up the mean firing rate in recordings while most neurons in the population are almost silent, some with rates as low as 0.1 Hz \cite{Kerr_2005}, whose activity might not even be picked up by most recording devices.
Although theoretical analysis of the distribution of firing rates in randomly recurrently connected networks of LIF neurons near the fluctuation-driven regime suggests considering mean firing rates around 6.4 Hz \cite{Roxin_2011}.
Based on the review of Wohrer et al. \cite{Wohrer_2013}, particularly on the firing rate in motor areas of behaving macaques, we decided to kickstart biological neural populations activity up to a conservative baseline firing rate of 1 Hz and study the neural dynamics of circuit input firing rates of up to 10Hz.

There are two parameters governing transmission of neural activity between neural populations.
First, the synaptic efficacy of connections, which was setup to be uniform across the circuit under the lack of appropriate hypothesis to tinker it in a detailed manner. According to London \cite{London_2002}, current understanding of synapsis is limited and contextual measurements and parametrization of efficacy might be more appropriate than fixing individual connection parameters.
For example recent evidence \cite{Briggs_2013} shows that synaptic efficacy might be modulated by attention processes.
In the study of Briggs \cite{Briggs_2013} neurons of the thalamus were stimulated while measuring evoked responses from corresponding monosynaptically connected neurons in primary visual cortex.
With this procedure the authors showed that, the percentage of shocks that evoke a postsynaptic response, the average efficacy, ranged from 28\% to 36\% depending on the type of neurons considered and the attention state.
Considering the possible efficacy variability in cortex, we decided to verify, through simulations of a sub-circuit, the sensitivity of the circuit temporal dynamics to low (10\%) and high (30\%) values of synaptic efficacy, where percentages are taken with respect to the difference between equilibrium and threshold potential, for both LIF and AdEx populations.

The second parameter governing transmission of neural activity was the number of connections between a pair of neural populations.
Unlike synaptic efficacy, the number of connections were determined from a series of simulation experiments.
First the number of connections from baseline persistent activity was set such that, during rest, the circuit steady state activity would stabilize around 1 Hz.
The number of baseline connections necessary is a function of input firing rate, synaptic efficacy and neural model, such that a lower synaptic efficacy required a higher number of connections.
Then the number of connections coming from excitatory populations was determined such that bidirectional gating circuits would have a stable steady state firing rate when both Gs allow neural activity to be transmitted.
Finally the number of connections coming from inhibitory nodes were setup high enough to block neural activity flow in a gating circuit, which means that GKs driven by MAs would be able to completely inhibit activity in Gs.
Our simple approach to neural rate transmission ignores many intricacies like activity regimes that might allow rich internal computations.
\cite{Ostojic_2014}.
Also connections distribution might have an impact in spike based communication \cite{Teramae_2012}.
Still we decided to keep connections between populations as simple and homogeneous as possible for a first approach.

\subsection{Simulation experiments performed}\label{simulation-experiments-performed}

Since it is possible to tune the circuit to reproduce a wide range of firing rate absolute values under which circuit dynamics are similar and stable, we simply aimed at picking reasonable parameter values such that the circuit would maintain overall modest firing rate values with respect to the literature of neural measurements.
To setup parameters and compare in detail the compartment circuit dynamics for LIF and AdEx neural populations, four simulation experiments were performed taking different sub-circuits into account.
A diagram of each sub-circuit is shown in Figure \ref{sub_circuits}.

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.9\columnwidth]{figures/sub_circuits}
    \caption{Sub-circuit simulation topologies.
      For better visualization baseline activity nodes are excluded from the topologies.
      A. Single neural population driven by baseline activity.
      This topology~ reminds of the fact that all MA, SA, G and GK populations are driven initially in the same way by a persistent baseline fixed rate.
      B. Chain of populations where activity is temporally interrupted by a control node.
      C. Excitatory loop between SAs when Working Memory is activated.
      D. Excitatory loop broken thanks to GKs inhibition. {\label{sub_circuits}}%
    }
  \end{center}
\end{figure}

The first simulation simply consists of the activity of one neural population driven by a fixed activity rate of 1 Hz.
We used this simulation to explore the necessary number of baseline connections to drive baseline activity in the circuit to approximately 1 Hz.
The second simulation allowed us to explore how neural activity flows through a chain of neural populations being regulated by a control mechanism.
The third simulation explores how neural activity is enhanced by a closed loop between a MA and SA, since it will be the case in the memory sub-circuit that activity is allowed to flow bidirectionally once the WM delay activity is unleashed.
Finally the fourth simulation consists on adding GKs to the closed loop sub-circuit of the second simulation to explore how many inhibitory connections are necessary to keep activity from flowing in the circuit unless the control mechanism allows it.

After determining reasonable parameter values, we simulated the complete circuit, shown in Figure \ref{fig:circuit-spec}, for both LIF and AdEX neural populations.
Then we compared the resulting neural patterns of the MA, SA, G and GK neural populations to binding and constituency effects available in the neuroimaging literature.

We simulated the binding activity related to the processing of complete phrases, by assuming a syntactic tree structure given by a phrase grammar and the order of control events given by a bottom up parsing scheme.
As a first simplified approximation to the NBA dynamics, we instantiated the required compartment circuits independently to represent the complete assumed tree structure and temporally align their neural signals according to input onsets.
Like this we obtained entire phrase neural time series, by summing activity across similar node categories of the multiple independent compartment circuits instantiated.
We used this procedure to simulate the neural activity of simple phrases, corresponding to increasing size right branching tree structures, to be compared with two different neuroimaging signals.

First, we showed similarities between the activity of simple phrases and ECoG time series patterns of binding revealed by Nelson et al\cite{Nelson_2017}. 
We naively compared the firing rates of our simulation directly to the patterns observed in ECoG recordings, considering the correlation that exist between the high gamma power of local field potential signals and firing rates\cite{Ray_2011,Manning_2009}.
Nonetheless a quantitative comparison would require a more careful consideration, employing recent models tuned to electro-physiological measurements that offer a way to translate neural activity to local field potentials\cite{Mazzoni_2015,Hagen_2015}.

Second, we concatenated simple phrases to reproduce the stimuli of Pallier \emph{et al.} (2011)\cite{Pallier_2011}.
Then we convolved the stimuli neural time series with the Glover Hemodynamic Response Function\cite{Glover_1999}.
This allowed us to make a qualitative comparison with the hemodynamic constituency effects depicted by Pallier et al. (2011)\cite{Pallier_2011}.

Since the quantitative level of neural activity can be easily tuned for a wide range of parameter values with similar behavior, when comparing the circuit neural dynamics with the neuroimaging literature, we only focused on the qualitative neural temporal patterns observed.
All the C++ scripts behind the circuit and sub-circuit simulations, taking advantage of the MIIND software\cite{de_Kamps_2008}, are accessible in the Blackboard application folder of the MIIND github repository at https://github.com/dekamps/miind.


\section{Results}

{\label{765620}}

\subsection{Sub-circuit simulations}

{\label{152250}}

\subsubsection{Experiment 1: Simple neural population}

{\label{461536}}

In the first experiment we explored the steady state rate and temporal behavior of the different neural models with different synaptic efficacies.
As indicated in the circuit topology of Figure \ref{fig:baseline_dynamics_lif}, neural populations were driven by a persistent 1 Hz input rate.
We show the steady state rate as a function of the number of baseline connections in the top plots of each neural model in Figures \ref{fig:baseline_dynamics_lif} and \ref{fig:baseline_dynamics_adex}.
In the bottom plots we display the respective firing rate dynamics for different number of connections.


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.00\columnwidth]{figures/baseline_dynamics_lif.png}
    \caption{LIF Baseline neural dynamics.
      The plots at the top show how the steady state rate of a neural population relates to the number of baseline connections for a baseline input of 1Hz.
      The plots at the bottom show the temporal dynamics for different number of baseline connections.}
    \label{fig:baseline_dynamics_lif}

  \end{center}
\end{figure}


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.00\columnwidth]{figures/baseline_dynamics_adex.png}
    \caption{AdEx Baseline neural dynamics.
      The plots at the top show how the steady state rate of a neural population relates to the number of baseline connections for a baseline input of 1Hz.
      The plots at the bottom show the temporal dynamics for different number of baseline connections.}
    \label{fig:baseline_dynamics_adex}

  \end{center}
\end{figure}


In the case of a LIF population, by manipulating the number of connections, we can tune to any value the steady state rate.
For all synaptic efficacy values, the firing rate increases smoothly until achieving the steady state at approximately 200 ms.
The AdEx population has a different temporal behavior.
An immediate transient peak of activity on initial stimulation is driven down by adaptation, achieving a steady state at approximately 600 ms.
The adaptation effect, on a 30\% synaptic efficacy, limits the range of values that the steady state rate can take by manipulating the number of connections.

As explained in the Methods section \ref{sec:circuit-parameters},
binding takes place in the Compartment Circuit when the kickoff input rate threshold of the Working Memory (WM) population is reached.
The total input rate of WM depends on the sum of the firing rate of both Sub-Assemblies in the Compartment Circuit, which themselves are driven by separate input events.
Since steady state rate values are limited in the AdEx model with high synaptic efficacy, operation of the circuit would be more constrained with non simultaneous input events, than in the low synaptic efficacy case.

Because we wanted to explore the behavior of the Compartment circuit for all possible timings of input events, we decided to restrict all remaining simulations to a 10\% synaptic efficacy.
We also fixed the number of baseline connections to 115 and 1646, for LIF and AdEx populations respectively, since these values best approximated the desired 1Hz steady state firing rate under a 10\% synaptic efficacy.


\subsubsection{Experiment 2: Neural activity flow and control
  release}\label{sec:experiment-2}

For the second experiment we wanted to understand how firing rate, in the Sub-assemblies of the Compartment Circuit, would vary with the timing of the onset of input and control events.
To accomplish this we employed the sub-circuit topology presented in Figure \ref{fig:experiment-2}.
In this topology the Gate (G) population is permanently inhibited by a Control (Ctl) population with persistent activity, such that the Sub-Assembly (SA) can not be driven by the Main-Assembly (MA) until a control event, that inhibits the Control population, takes place.
For this experiment, the number of excitatory connections was fixed to 9 for LIF populations and 20 for AdEx populations.
The effect of modifying the number of excitatory connections will be explored in Experiment 3 in Results section \ref{results:experiment-3}.


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.0\columnwidth]{figures/control_subcircuit}

    \caption{Neural dynamics of input and control events.
We plot the temporal dynamics of the Sub Assembly population corresponding to the sub-circuit topology shown.
9 and 20 excitatory connections are assumed for the LIF and AdEx models respectively.
We show the time series after 1000 ms, time at which all neural populations have achieved a steady state rate from their initial events at time 0.
For each neural model two constant input rates are simulated for the input events, 10 Hz and 20 Hz for LIF, and 20 Hz and 30 Hz for AdEx.
There are three possible extreme cases of timing between the input and control events;
When the input event takes place at 0 ms and the control event at 1000 ms (Input First);
When both events start at 1000 ms (Simultaneous);
And when the control event starts at 0 ms followed by the input event at 1000 ms (Control First).
}
    \label{fig:experiment-2}
  \end{center}
\end{figure}


We considered two possible persistent rates for the input event, 10 Hz or 20 Hz for the LIF model and 20 Hz or 30 Hz for the AdEx model.
We needed higher input rates for the AdEx model since adaptation induces smaller steady state rates with respect to the LIF model.
There are three possible extreme cases of timing between the input and control events;
When the input event takes place at 0 ms and the control event at 1000 ms (Input First);
When both events start at 1000 ms (Simultaneous);
And when the control event starts at 0 ms followed by the input event at 1000 ms (Control First).
These timing of events are extreme cases because 1000 ms is enough time for the neural populations to achieve a steady state rate after any event initiated at 0 ms.
Any other timing in which populations have still not achieved a steady state before the arrival of the second event would produce neural dynamics with patterns in between the extreme cases.
For language stimuli, timing cases can be interpreted as different types of parsing mechanisms, where Control First corresponds to a predictive (top-down) one and Simultaneous and Input First to a reactive (bottom-up) one.
We show in Figure \ref{fig:experiment-2} the firing rate time series of the Sub-Assembly (SA) for all possible event timing cases and input firing rates.

First we observe that the input rate do not change the relative behavior of the timing cases but only increase the steady state rate and transient fluctuations.
We see that the timing cases do not modify the final steady state rate, which only depends on the input rate, but influence the maximum rate of the transient activity fluctuations.
In the case of AdEx, the speed at which the steady state is approximated is also affected by the timing cases, for example the Simultaneous case takes approximately 400 ms more than the Control First case, to achieve the steady state, for a 30 Hz input rate.
The steady state rate is in most cases and neural models the lowest firing rate, with some short transient exceptions.
Moreover the timing cases have different relative behaviors depending on the neural model, as can be seen from the Control First case that has the lowest transient rates for AdEx but the highest ones for LIF.

Successful binding in the Compartment Circuit depends on the sum of activity of two SAs, that reaches the kickoff threshold rate of the Working Memory (WM) population.
Assuming activity of SAs is driven by two separate input events, like two words to be binded presented 200 ms apart, the timing of the two input events and the timing cases of their respective control events will determine the possible range of values for the WM kickoff threshold.
We can also think the other way around and say that the range of values of the WM kickoff threshold constrain the possible timing of all events.

An example scenario, illustrated in Figure \ref{fig:experiment-2} for a LIF population with 20 Hz input, would be that the onset of input events correspond to the onset of word presentation, 200 ms apart, where the timing of the first SA input event follows the Input First case and the timing of the second SA input event follows the Control First case.
In that scenario any WM kickoff threshold between 16 Hz and approximately 44 Hz would be reached by the sum of the 15 Hz steady state rate of the first SA and the firing rate of the second SA achieving a transient maximum of approximately 29 Hz.

Since we wanted to consider all possible event timings when studying the Compartment Circuit dynamics, we took from this experiment the cases with the highest transient rates for each neural model, to later analyse the circuit parameter space.
We see in Figure \ref{fig:experiment-2} that the Control First case has the highest transient rate for the LIF model, while the Simultaneous case has the highest transient rate for the AdEx model.


\subsubsection{Experiment 3: Circuit operation according to the parameter space}

{\label{results:experiment-3}}

In a third experiment, we studied the parameter space of the input rate, the number of excitatory connections and the WM kickoff activation threshold, to understand the operational, event timing related, constraints of the Compartment Circuit when attempting to instantiate binding under different regions of the parameter space.
As shown in Figure \ref{fig:experiment-3}, to explore the circuit behavior, we have to consider the Sub-Assembly (SA) temporal dynamics presented in Results section \ref{sec:experiment-2} and a sub-circuit topology representing an excitatory loop between two SAs.


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.\columnwidth]{figures/parameter_regions}

    \caption{Excitatory loop and WM activation parameter regions.
At the top the two sub-circuit topologies from which SA firing rate curves are derived.
Rate curves consist on firing rate as a function of the number of excitatory connections for a given input rate of 10Hz and 25Hz for the LIF and AdEx models respectively.
From the chained neural population topology we consider the highest maximum transient rate and the steady state rate.
From the excitatory loop topology we consider the steady state rate driven only by baseline activity.
We color the regions between the curves to indicate the different WM activation cases determined by the value of the WM "half" kickoff threshold rate.
The four parameter regions refer to the possible combination of input and control events that would allow binding to take place if the WM "half" kickoff threshold falls in the region:
The perpetual activation region implies that WM will get permanently reactivated;
The flexible activation region implies that all events cases can produce binding;
The constrained activation region implies that only some combination of events' timings can permit binding;
Finally the impossible region implies that no binding can take place for the given WM kickoff rate.}
    \label{fig:experiment-3}

  \end{center}
\end{figure}


As shown in the Compartment Circuit diagram of Figure \ref{fig:circuit-spec} of Methods section \ref{sec:circuit-parameters}, once the Working Memory (WM) Delay activity is unleashed, both Gate Keepers (GKs) are inhibited, creating an excitatory loop between the Sub-Aseemblies (SAs).
Beyond a certain number of excitatory connections, there is the possibility of runaway activity in the excitatory loop, which motivates a constraint in the parameter space of the Compartment Circuit.
The excitatory loop activity considered is only driven by the 1 Hz baseline input rate, as would be the case in the circuit once the input events stop driving activity in Main-Assemblies (MAs) and as a consequence in SAs.
In Figure \ref{fig:experiment-3} we plot the space of excitatory connections up to 11 connections and 21 connections for LIF and AdEx respectively, values at which we observed runaway activity in the excitatory loop.

Alongside the excitatory loop baseline steady state rate curve of the SA, we also plot the input driven maximum transient firing rate and steady state rate of an SA, according to the different events' timing behavior presented in Results section \ref{sec:experiment-2}.
The firing rate curves correspond to an input of 10 Hz and 25 Hz for LIF and AdEx populations respectively.
All the firing rate curves correspond to the activity of only one SA, so whenever we represent the WM kickoff rate threshold in Figure \ref{fig:experiment-3}, we refer to the "Half" kickoff threshold.
For example the convenient "Half" kickoff rate threshold of 6 Hz, marked with a green line in the LIF Model plot, implies a total WM kickoff rate threshold of 12 Hz.

From the relationship between the three firing rate curves, we can establish four parameter regions with different implications for the behavior of the Compartment Circuit:
First, below the excitatory loop baseline steady state rate, we have a parameter region for which WM would be continuously reactivated.
The initial activation of WM leads to the excitatory loop steady state rate, so if the kickoff threshold is below it, WM will be reactivated perpetually.
We call this the WM perpetual activation region;
Second, in the area between the loop steady state and the input steady state curves, all input and control event timing cases will lead to activation of WM, which can be explained by the steady state rate being the lowest transient rate.
We call this the WM flexible activation region;
Third, in the region between the input driven maximum transient rate and the steady state rate curves, activation of WM will not take place for some timings of input and control events.
The higher the WM kickoff threshold in this region, less input and control event timing cases can activate WM.
We call this the WM constrained activation region;
Finally, above the input driven highest maximum transient rate, it is clear that activation of WM can not be achieved under any circumstance, which is why we denote it as the WM impossible activation region.

To understand the constrained activation region, it helps to take a look back at Figure \ref{fig:experiment-2} of Results section \ref{sec:experiment-2}.
Consider the AdEx model with a 30 Hz input rate.
We can see that a WM kickoff rate of 14 Hz would be reached by adding the steady state of one SA and the transient rate of any events' timing case for the second SA.
If we raise the WM kickoff rate to 20 Hz then we would need the events driving the second SA activity to follow the Input First or Simultaneous timing cases, while raising it further to 25 Hz would leave the Simultaneous case as the only option.

We still do not know the parameter variability allowed by the cortex to implement the circuit, so we consider the proportion between the constrained and flexible activation parameter regions as a indicator of the difficulty to operate the Compartment Circuit under the different neural models.
Based on this, we observe in Figure \ref{fig:experiment-3} that the AdEx model is more likely to induce constraints in the timing of input and control events to perform the bindings necessary to represent complete structures in cortex.
To allow the most flexible behavior exploration of the Compartment Circuit, when simulating language processing, we decided to select parameters in the flexible activation region.
We selected a combination of 10Hz and 20Hz input rates, 8 and 20 excitatory connections and 10Hz and 9Hz WM kickoff rates for LIF and AdEx populations respectively.


\subsubsection{Experiment 4: Inhibition of undesired excitatory loops}

{\label{554287}}

In the fourth experiment, we aimed at tuning the amount of inhibitory connections between Gate Keepers (GKs) and Gates (Gs) to avoid excitatory loops between Main-Assemblies (MAs) and Sub-Assemblies (SAs).
We also wanted to reduce the sensitivity of the circuit to input fluctuations and the number of excitatory connections when the Gs are supposed to be closed.
So we decided to study this with the sub-circuit topology of Figure \ref{fig:experiment-4}.


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.0\columnwidth]{figures/experiment_4}
    \caption{Inhibition to avoid excitatory loop.
The sub-circuit topology at the right depicts the inhibition of Gates (Gs) by the Gate Keepers (GKs) that are driven themselves by the Main and Sub Assemblies (MA and SA) to avoid an excitatory loop between them.
Activity in the sub-circuit is driven only by a 1 Hz baseline rate.
Each curve in the plots represent how the maximum transient rate of SA for a given number of excitatory connections varies as we increase the number of inhibitory connections.
We present one plot for each neural model (LIF and AdEx).
The maximum firing rate is employed instead of the steady state rate to observe sensitivity to transient rate fluctuations.
}
    \label{fig:experiment-4}
  \end{center}
\end{figure}


We plot, in Figure \ref{fig:experiment-4}, the maximum transient firing rate of the SA as a function of the number of inhibitory connections for each number of excitatory connections.
If the amount of inhibitory connections is not enough, transient activity of the SA will be increased by the effect of the excitatory loop.
So it is possible to see how many inhibitory connections are necessary to eliminate the undesired excitatory loop effect between an MA and SA, by looking at the amount of inhibitory connections at which the maximum firing rate becomes insensitive to the number of excitatory connections.
It is clear from the plots, that after a certain number of inhibitory connections, the excitatory loop is eliminated, such that unidirectional activity will be allowed only by controlled inhibition of the GKs.
From this experiment, we decided to set the number of inhibitory connections to 70 and 250 for LIF and ADEX populations respectively.


\subsection{Complete compartment circuit simulations}
{\label{444332}}

After selecting a valid set of parameters from the observations done in the sub-circuit experiments, we analysed the behavior of the complete compartment circuit simulation.
The dynamics of the compartment circuit can be summarized by a combination of the input events that drive activity in Main-Assemblies (MAs) and the control events that inhibit Gate Keepers (GKs) such that activity can flow from MAs to Gates (Gs) and from the latter to Sub-Assemblies (SAs).
In Table \ref{table:simulation-parameters} we present a summary of the parameters taken for LIF and AdEx simulations and in Figure \ref{fig:activity_profiles} we present the temporal dynamics of the compartment circuit for a complete and incomplete binding.


\begin{table}[h!]
  \centering \normalsize
  \begin{tabular}{lcc}
    \textbf{Parameter} & \textbf{LIF} & \textbf{AdEx} \\
    baseline connections & 115 & 1646 \\
    excitatory connections & 8 & 20 \\
    inhibitory connections & 70 & 250 \\
    Input rate (Hz) & 10 & 20 \\
    WM/Ctl rate (Hz) & 10 & 20 \\
  \end{tabular}
  \caption{Complete simulation parameters}
  \label{table:simulation-parameters}
\end{table}


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.0\columnwidth]{figures/compartment_circuit_dynamics}
    \caption{Profiles of neural activity.
      \textbf{A.} Neural activation driven only by baseline input. \textbf{B.} Neural activation of the circuit when only one MA is activated by a word event or WM at 500ms.
      Shows the neural activity related to an erroneous control signal at 800ms.
      It is possible to see that the steady state of neural activity is resilient to a slip of control, going to the appropriate levels of neural activity once the control activity is over.
      \textbf{C.} Neural activity of the Compartment Circuit for a successful binding.
      The second MA gets activated at 800ms alongside the controls.
      Since both MAs are active, the SAs manage to activate WM to instantiate the binding of the MAs.
      Two interesting dynamics arise from the binding:
      The first is that a spike of activity in SAs, GKs and Gs takes place due to the sudden inhibitory activity of WM on the GKs;
      The second is that the memory circuit internally raises its baseline activity due to the excitatory loop formed.
    }

      \label{fig:activity_profiles}
  \end{center}
\end{figure}


First, we show the baseline dynamics of the circuit when no event takes place in part A of figure \ref{fig:activity_profiles}.
In this case all neural populations are only receiving an input baseline rate of 1 Hz.
So the different populations just reflect with their firing rate the architecture of the circuit.
Gs show a low rate of activation due to GKs inhibition, while GKs show the highest rate driven by MA and baseline activity.
MAs show an activation close to the approximated 1Hz baseline as well as SAs that have been isolated in the circuit thanks to GKs inhibition.

Second, we show the activity of the circuit for an incomplete binding in part B of Figure \ref{fig:activity_profiles}.
This means that only one MA is driven by an input event, after which a Control (Ctl) event allows activity flow from both MAs to SAs, even though there is no binding to be done.
Due to stimulation of the MA, the GK firing rate raises to stop activity to flow to the SA until the control event takes place to inhibit the GK.
As only one SA is driven by input, the total rate contribution to the WM population do not achieve the WM kickoff threshold rate necessary to perform a binding.
Both neural models display a transient spike of neural activity in the SA, G and memory sub-circuit GKs during the time window the control permits activity to flow to the SA.
The AdEx model also shows a high transient firing rate in the GK after the control stops, due to the negative adaptation of control inhibition.

Third, we show the circuit dynamics of a successful binding in part C of Figure \ref{fig:activity_profiles}.
When both MAs are driven by an input event and a control event takes place.
In this case the added activity of the SAs reaches the WM kickoff threshold and kickstarts the Delay activity of WM.
Then activity in the SAs and Gs of the memory sub-circuit raise to a new baseline due to the excitatory loop created by WM inhibition of GKs, which also generates an initial transient spike of activity in SAs.
A similar behavior to this one, simulating sentence parsing, was also reported by previous work with the NBA\cite{Frank_2014}.
Finally, after the WM Delay activity stops, the LIF model activity goes back to baseline, but the AdEx model exhibits a final transient rise of firing rate in the GKs of the memory sub-circuit, similar to that of the GKs affected by control inhibition release.


\subsection{Simulation of complete phrase processing}
{\label{44kfmvjkc32}}


With the neural dynamics of several Compartment Circuits, we approximated the binding of complete phrases.
The right branching hierarchical structure that corresponds to an example phrase of 4 words, determined by a phrase grammar, is shown in Part A of Figure \ref{fig:tree-example}.
In this example only three Compartment Circuits are necessary to realize all the bindings that would correspond to the phrase processing, and the exact input event onsets were taken from the LIF simulation.
The onset of input events driving Main Assemblies that represent word grammatical categories were matched to word presentation onsets spaced 600 ms apart from each other.
In the case of phrasal nodes, we assumed that their input event onset correspond to the previous realization of a binding, determined by the moment at which their respective Working Memory population was activated.
In this way, phrasal nodes can be represented by activity in the Main-Assemblies of a Compartment Circuit and be binded to other word grammatical categories or phrasal nodes.


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.00\columnwidth]{figures/compartments_tree_example}
    \caption{Sentence processing example.
      A. Tree structure hypothesized for a given 4 words phrase.
      It is shown how compartment circuits correspond to sections of the tree structure and how the nodes corresponding to grammatical categories of words processed or phrase nodes are instantiated in time under a bottom-up parsing approach.
      B. Blackboard time series that correspond to the simulated processing of the considered tree structure and time of activation of the nodes.
      The separate activity of the LIF populations of each compartment circuit are shown separately, followed by their summary and total activity.
      C. Same as~B but for AdEx populations. {\label{fig:tree-example}}%
    }
  \end{center}
\end{figure}


We needed to prolong the Main-Assemblies and Working Memory activity long enough to instantiate all the necessary bindings, so in this example we assumed WM and input events to last 2300ms for all simulations.
As indicated in the second phrasal node (PN2) of the tree example, if input events were active for less than 1972ms then activation of the first word MA would cease before the accompanying phrasal node MA comes into play to realize the last binding.
In the Comparment Circuit simulation presented in Figure \ref{fig:activity_profiles}, there was a difference in timing of WM activation between the LIF and the AdEx neural models, that was not easy to see in the plots.
The Working Memory population became active 86 ms after all input and control events take place in the LIF simulation, while in the AdEx simulation this only took 42ms.
By contrasting the LIF and AdEx complete phrase simulations in Figure \ref{fig:tree-example} we can better appreciate how this difference adds up to accelerate phrase processing in the AdEx model.

To later compare the phrase processing simulation with neuroimaging patterns, we first substracted baseline activity from the time series of each neural population in each Compartment Circuit.
Then we summed the aligned time series of the same neural population category belonging to different Compartment Circuits.
Finally, to obtain total neural activity of phrase processing, we summed activity from all the non phenomenological neural populations and the Working Memory population, such that they would all be equally weighted under the absence of a more detailed hypothesis about the neural population sizes and their spatial distribution in the cortex.


\subsection{Qualitative reproduction of ECoG patterns}
{\label{sec:ecog-patterns}}


As presented in the Introductory section \ref{intro:imaging}, the ECoG analysis of Nelson et al.\cite{Nelson_2017} is the first to characterize the specific temporal patterns of phrase-structure formation from intracranial neurophysiological data, possibly revealing the first neural signatures of binding operations.
Nelson et al. demonstrate two patterns that are of particular interest to our simulations; The average temporal dynamics of processing increasing size right branching phrases; And the average neural dynamics for hypothesized number of pending binding operations, during phrase processing, under a bottom-up parsing approach.
In Figure \ref{fig:ecog-1} we show the aggregated neural activity predicted by our LIF and AdEx simulations, alongside the temporal dynamics of phrase processing presented by Nelson et al. from the mean high gamma power of the intracortical recordings.


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.70\columnwidth]{figures/ecog_comparison_1.png}
    \caption{{Simulation comparison with intracortical (EcoG)
        recordings.
        In the top plots we show phrase processing for the LIF and AdEx simulations.
        We denote with arrows the four segments of neural dynamics identified in the simulations; The Main Assemblies (MAs) activity increase the segment; The accumulated binding operations segment; The Main Assemblies (MAs) activity release segment; And the Working Memory (WM) release segment.
        We denote with red bars the magnitude of Working Memory activity in the circuit that depends on phrase length and remains at the end of phrase processing.
        In the bottom plots we identify, in Figures modified from Nelson et al., the segments of intracortical recordings that resemble the simulation and denote with red bars the possible Working Memory related activity that remains at the end of phrase processing.
        {\label{fig:ecog-1}}%
      }}
  \end{center}
\end{figure}


As can be seen in the top plots, our simulations suggest the existence of four qualitative different segments of neural dynamics;
First, as words are presented to the circuit, input events drive activity in Main-Assemblies (MAs) corresponding to the grammatical categories of the words.
The activity of all the MAs accumulate but still do not change the activity of other neural populations on the Compartment Circuits, since for parsing a right branching tree under a bottom up parsing scheme, control events that allow bindings do not occur until the last word is presented;
The second segment correspond to the succession of bindings that take place after the last word of the phrase is processed.
The neural activity allowed by the control events creates a transient rise in activity that stabilizes with the accumulated Delay activity of the Working Memory populations and the still ongoing input activity;
The third segment is characterized by the gradual drop of input related activity;
And the fourth segment corresponds to the final drop of Working Memory activity, such that all the neural populations return to their baseline steady state rate.

We see in the bottom plots of Figure \ref{fig:ecog-1}, modified from the Figures in Nelson et al., that we can identify qualitatively the three initial segments predicted by the simulation in the high gamma power time series.
We observe an initial increase in neural activity, for which a later onset and higher magnitude of the peak appear to depend on phrase length, as would be explained by the first segment of the simulation based on an increase of activity in Main Assemblies (MAs).
The following transient fluctuations of the ECoG time series could be identified with the binding related segment and the final activity drop with the release of MA activity.
Contrary to the paced drop of MA activity of the simulation, the ECoG time series suggest a more abrupt drop after bindings have taken place, which complicates distinguishing the neural fluctuations related to the binding operations from those related to the MAs activity release.
In the longer 6 words phrase "Ten sad students of Bill Gates" there is a middle sentence high transient fluctuation that is not expected from a bottom up parsing scheme.

We indicate with red bars, that the activity drop of the ECoG time series stops at a higher level than the initial baseline, which is compatible with the hypothesized ongoing Working Memory (WM) activity of the simulation.
The AdEx model distinguishes itself from the LIF model, during WM inactivation, by predicting a final burst of activity due to the inhibition release of the Gate Keepers in the memory circuit.
Nonetheless, due to the task of the ECoG experiment, that requires retaining in memory the phrase for later comparison with another phrase, we should not be able to observe the final drop of WM activity predicted by the simulation, as is the case.

In Figure \ref{fig:ecog-2} we show, in the top plots, the simulation time series aligned on the last word onset, to better demonstrate the neural activity fluctuations linked to the number of accumulated and executed binding operations, which Nelson et al. refer to as the number of nodes closing.
In the bottom plots we show modified Figures from Nelson et al. in which the effect is demonstrated in the case of middle sentence operations and sentence end operations.


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.70\columnwidth]{figures/ecog_comparison_2.png}
    \caption{{Effect of number of executed binding operations.
        In the top plots we show the phrase processing time series of the LIF and AdEx simulations, aligned on the onset of the last word.
        We denote with arrows the segment of transient rise and drop of neural activity hypothesized to be linked to the number of executed pending binding operations, which we refer to as number of nodes closing in the plots, following terminology from Nelson et al.
        In the bottom plots we show, in Figures modified from Nelson et al., the intracortical recordings effect of executed pending binding operations at the middle and end of phrases.
     {\label{fig:ecog-2}}%
      }}
  \end{center}
\end{figure}


\subsection{Qualitative reproduction of Bold-fMRI patterns}
{\label{sec:pnas-patterns}}

As explained in the Introductory section \ref{intro:imaging}, we also reproduced patterns from an experimental design employed to show constituency effects with Bold-fMRI\cite{Pallier_2011}. 
Stimuli, presented to a subject in a trial, consisted of a list of phrases with the same number of words (constituents), such that in total 12 words would be presented.
All phrases correspond to right branching trees according to the phrase grammar considered by the authors.
The conditions were one list of 12 unconnected words (c01), 6 phrases of 2 words (c02), 4 phrases of 3 words (c03), 3 phrases of 4 words (c04), 2 phrases of 6 words (c06) and 1 phrase of 12 words (c12).

Besides normal words, the design also included pseudoword conditions that maintained morphological markers and closed-class (function) words.
We will compare our simulation with the pseudoword effects of Pallier et al, since they provide syntactic specific patterns that can be interpreted closer to the abstract binding operations of our simulation.
Moreover we continue to assume the same phrase grammar and bottom-up parsing scheme employed for comparison with the intracortical recordings of Nelson et al.
To simulate the Pallier et al. stimuli, we added the repeated neural time series of each of the right branching trees in a condition.
So, for example, to simulate the 4 phrases of 3 words condition (c03), we aligned and summed, based on word onsets, the neural activity of 4 simulations of a 3 words phrase.

In the standard analysis of Bold-fMRI time series, events are modelled as a constant stepwise function that reflects the duration of the stimuli, called a boxcar model.
The boxcar model events are then convolved by an Hemodynamic Response Function (HRF), for which we considered the HRF proposed by Glover\cite{Glover_1999}, available in the python open source package Nistats \footnote{https://github.com/nistats/nistats}.
The convolved events are then used in a general linear model (GLM) to obtain a peak estimate of hemodynamic responses for the different conditions, as was the done in the Pallier et al. study.


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.00\columnwidth]{figures/pnas_comparison-01}
    \caption{{Hemodynamic interpretation of the simulation.
    At the top and middle plots we show the rescaled time series of the LIF and AdEx simulations respectively, alongside the HRF convolved time series.
    At the bottom we show a boxcar event of 3600 ms and its convolution, as was employed by Pallier et al. to estimate the amplitude of responses for the different conditions from the Bold-fMRI timeseries.
    we considered the HRF proposed by Glover, available in the open source python package Nistats.
     {\label{fig:pnas-1}}%
      }}
  \end{center}
\end{figure}


We generated a prediction of hemodynamic responses from our simulations by rescaling the conditions' time series by the maximum firing rate of all conditions and then convolving them with the HRF.
We present the predicted hemodynamic responses in the top and middle plots of Figure \ref{fig:pnas-1}.
Since in the Pallier et al. study, 12 words are presented every 300 ms, we considered the last word onset of 3600 ms as the duration of the stimuli for a traditional boxcar event model, shown in the bottom plots, to compare it with our models.
We mark the HRF peak and its onset with black lines on all the HRF convolved time series.

We observe that the neural time series would predict in all cases a peak onset displaced many seconds with respect to the traditional boxcar event that only represents the duration of the stimuli.
Looking at the time series, this would be expected, since the HRF peak onset depends on the center of mass of the accumulated neural activity, which continues several seconds after the last word onset in our simulations.
The peak onset in the LIF and AdEx models follow a super-linear increase with respect to the number of constituents, at odds with with sub-linear patterns reported by Pallier et al.
Also the LIF neural model introduces an slightly longer onset delay with respect to the AdEx neural model, due to its slower activation of Working Memory populations.


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1.00\columnwidth]{figures/pnas_comparison-02}
    \caption{{Hemodynamic peak magnitudes comparison with Bold-fMRI experiment.
    The top plots show the number of bindings executed for each condition alongside the rescaled Hemodynamic Response Function (HRF) amplitudes of each of the Compartment Circuit neural populations.
    We demonstrate that the hemodynamic pattern of the neural populations in the simulation follow closely the number of bindings executed.
    In the bottom plots we contrast the pattern of the total neural activity in the simulation alongside the sub-linear patterns reported by Pallier et al. in the pSTS, IFGorb and IFGtri brain regions.
        {\label{fig:pnas-2}}%
      }}
  \end{center}
\end{figure}


In the case of the HRF peak amplitudes, we show in Figure \ref{fig:pnas-2} that both LIF and AdEx models predict a sub-linear pattern of peak amplitudes as a function of the number of constituents.
We demonstrate in the top plots that the HRF magnitudes of added neural activity in all neural populations of the Compartment Circuit follow the pattern given by the number of executed bindings in a condition.
It is unlikely then, that the sub-linear pattern appreciated in the HRF amplitudes would be qualitatively changed by manipulating other parameters of the circuit, like the duration of the input to Main-Assemblies and Working Memory that could modify qualitatively the peak onsets pattern.

Pallier et al. reported constituent sub-linear responses in the language areas TP, aSTS, pSTS, TPJ, IFGorb and IFGtri, but only the regions pSTS, IFGorb and IFGtri showed a similar response pattern when minimizing the semantic content of phrases with pseudowords.
Since our simulation puts aside semantic considerations, we consider this type of experimental manipulation to be a better reflection of the binding activity modelled in the Compartment Circuit.
In the bottom plots of Figure \ref{fig:pnas-2}, we show the similarity between the HRF magnitude pattern of the total neural activity in the simulation models with what is reported by Pallier et al. in the pSTS, IFGorb and IFGtri brain regions.


\section{Discussion}
{\label{sec:discussion}}


\subsection{The neural models and circuit architecture}

Regarding the neural model parameters, we left for future work consideration of values based on electrophysiological recordings from specific brain regions, in contrast to the Omurtag\cite{omurtag2000simulation} and Brette et al. parameters \cite{Brette_2005} taken for a first approximation of the neural dynamics.
For example, there are different adaptation constants along the cortex, that could change the AdEx model dynamics.
Since we have compared the simulation with neural activity in specific brain regions like aSTS, pSTS, IFGtri and IFGorb, it would be reasonable to fit the simulations to their biological reality.

In the case of the Compartment Circuit assumptions, we made many simplifications that should be revised in future work.
We approximated baseline dynamics with a low constant input rate instead of considering the natural oscillatory activity of the cortex, homeostatic mechanisms in cortical circuits\cite{turrigiano2011too} and balanced networks\cite{Wolf_2014}.
Also we adopted homogeneous synaptic connections instead of testing different synaptic distributions that could have an impact in the neural dynamics.
If we allowed random connectivity to shape the Compartment Circuit architecture our capacity to control its dynamics with the number of connections would be restrained.

We would also suggest to extend the current neural population models with modular cytoarchitectonic considerations to allow a better spatial interpretation of the circuit activity in a cortex patch.
It might even be necessary to accurately translate firing rates into Local Field Potentials\cite{Mazzoni_2015,Hagen_2015}, hemodynamics\cite{Buxton_2004} and other neuroimaging measurements.

The explicit simulation of Delay Activity in Working Memory was left out of the current work due to its flexible and still debated implementation\cite{de_Kamps_2005}.
Studying it could reveal important neurobiological limitations on the way we asses the relative proportion of neural activity between Main-Assemblies and Working Memory.
Also it could provide a more limited set of hypothesis about the spatio-temporal memory limitations of the Neural Blackboard Architecture, to be compared with neuroimaging evidence.

A questionable decision on our circuit implementation was allowing for the existence of excitatory loops, due to the possibility of unstable runaway neural activity.
Neural activity related to these excitatory loops was regulated in the Working Memory sub-circuit by careful tuning of excitatory connections and in all the Compartment Circuit by additional tuning of inhibitory connections that would close the loops.
Instead of regulating the excitatory loop with the number of excitatory connections, we could provide additional control mechanisms to condition activity flow after the Working Memory population has been activated, in the same way its done for Main-Assemblies to Sub-Assemblies communication.
Nonetheless the second approach implies additional complexity in the number of nodes, connections and events we have to consider for the circuit operation.
Since we do not really know what is closer to the biological reality of the cortex, we decided to show that the less complex architecture including the excitatory loop could be made stable, but consideration of a more complex architecture would also be possible.

\noteMP{\emph{Should add paragraph here about: Also the reason we have this excitatory loops is because of the bidirectional gating circuit mechanism proposed. Add to discussion possible substitution of gating mechanism at the intraneural level. multiplicative incoming activity that act as gates in visual system with information coming from retina filtered by attention processes. we dont know if non visual circuits use the same mechanism. I am lacking to read a proper reference to develop this}}. 

To our knowledge, this is also the first time complex neural models like AdEx are simulated alongside LIF for variable binding and language function related circuits.
Thanks to this, we found that the circuit implementation and neural dynamics interpretation can depend on the underlying neural model in non trivial ways.
For example we observed that in a LIF model there was a non-consequential trade-off between synaptic efficacy and number of excitatory connections to control the steady state rates of the circuit.
On the other hand the AdEx model was very sensitive to changes in synaptic efficacy due to adaptation effects, to the point of making us unable to control the magnitude of the steady state rate of the circuit for high synaptic efficacy values.
If the physical reality of the cortex was closer to an AdEx model with high synaptic efficacies we would then need to restrict our hypothesis about the circuit operation with input and control events to a subset of the possibilities explored in our simulation.

Another important distinction observed between the AdEx and LIF model was how dynamics after inhibition are qualitatively different under the influence of adaptation.
While in the LIF circuit, neural activity on a population would smoothly recover back to its steady state after inhibition stops, that of an AdEx circuit would show a renovated burst of activity due to adaptation decreasing during the inhibition period.
The effect might be strong enough to suggest it as a predictive marker for certain events in the circuit, like the release of Working Memory activity.

Moreover, characterizing the Working Memory activation parameter regions was important to understand the reliability of the circuit when exposed to noisy input rates, arbitrary timing coordination of events, control mistakes or anticipatory control signals.
Although for a bottom-up parsing approach, we can safely assume control events to take place after input events, this might not be the case for other parsing strategies like top-down, that could be implemented with anticipatory control events.
Since some parameter regions restrict the timing of input and control events, we might get insights into the possible set of parsing mechanisms directly from the anatomical structure of the cortex that constrain the parameter boundaries.

Finally, the question of how a Compartment Circuit and the Neural Blackboard Architecture could be formed during brain development and modified by learning is still work in progress, partially tackled in a previous study\cite{van_der_Velde_2011}.
We do not expect the hard-wired architecture modelled here to represent the biological reality but to facilitate an approximation to its behavior.
Demonstrating how neural mechanisms approximated by the architecture can be implemented with biological realistic Hebbian or STDP rules alongside random connectivity constraints, during development and learning, would be an important avenue of future research.

\subsection{Circuit implications of the linguistic assumptions and hypothesis}

A strength of the current simulation is its flexibility to predict the neural activity of diverse grammar theories and parsing schemes, which we only explore partially in this work.
We could in principle, without circuit modification, predict the binding activity for any structure that can be represented by a binary tree.
This is the case of the phrase grammar of the minimalist program of Chomsky\cite{Chomsky_2014}, that represent phrases as binary trees, and also the case of other theories like dependency grammars\cite{nivre2005dependency} that represent grammatical relations between cords.
Nonetheless in the case of dependency grammars, as they do not require a hierarchical representation, we would not need to assume that the Working Memory of an executed binding drives the Main-Assembly of another Compartment Circuit.

Because we only modelled a bottom-up parsing scheme, we have considered activation of the Main-Assemblies corresponding to phrase nodes only as a result of a previous justifying binding.
For example for the phrase "the black cat" we would create an input event for the phrase node of "black cat" after "black" and "cat" have been binded.
If we consider instead a pure top-down parsing scheme, that implies prediction of future bindings, or the generalized left corner parsing scheme proposed by Hale\cite{hale2014automaton}, there would be three additional mechanistic options to consider:
First, we could start input events for Main-Assemblies representing the phrase nodes before their motivating bindings and only start the control event after the bindings have been confirmed;
Second, we could start the control events beforehand, which is an option explored in the simulation, and still make input events follow the motivating bindings;
Third, we could go ahead and perform bindings ahead of time, that would need to be deactivated by an error signal provided by the parsing mechanism. This last option would allow to simulate the possibility of multiple parallel phrase representations, from which only one survives at the end.

A simplification was made regarding the Compartment Circuit selection mechanism in the Neural Blackboard Architecture.
We did not model the dynamic inhibition of competing Compartment Circuits belonging to the same Connection Matrix.
To do it we would require an hypothesis about the size of the Neural Blackboard, governed by memory limitations and the total number of possible grammatical category combinations given by a grammar.
Forming such an hypothesis was out of the scope of this work, so we opted to assume the simplest selection mechanism possible based on uniform random selection, which is how we justify simply recruiting Comparment Circuits as needed.

Nonetheless we are only able to ignore the inhibitory activity of competing Compartment Circuits in complete Connection Matrices because we are not planning to explore the effects of memory limits under time compressed sentence processing scenarios or memory tasks.
Otherwise important deviations in background neural activity due to depletion of available Compartment Circuits and additional inhibitory activity would become a crucial factor for the simulation.
We plan to explore this in future work, to try to reproduce temporal bottleneck effects shown by Vagharchakian et al. on hemodynamic responses, based on a Bold-fMRI experiment with an experimental design containing compressed speech and reading conditions\cite{Vagharchakian_2012}.

With respect to the parsing mechanism, we only model its interface with the Comparment Circuit that implements binding, through the assumed control signals.
We considered that understanding how a parsing algorithm is learned and implemented by the cortex, such that it can provide the respective control signals, was a separate research question.
Previous work has shown the feasibility to implement a parsing mechanism with neural networks in connection to the Neural Blackboard Architecture\cite{van_der_Velde_2010}, for a limited set of possible syntactic structures.

As can be inferred from this discussion, there is already great potential for exploration of linguistic hypothesis with the current simulation developed, but there are also many open questions left for future development.
We believe that taking into account more experimental evidence from psycholinguistics and neuroimaging studies is necessary to guide future refinements of the circuit architecture and simulation.


\subsection{Qualitative reproduction of neuroimaging evidence}



Certainly the way we should aggregate time series to compare them quantitatively with neuroimaging data would depend on the spatial and temporal resolution available and the assumptions made about the spatial distribution of the blackboard architecture in the cortex.






The simulation results show how both the steady state and transient dynamics of the circuit affect our interpretation of the neuroimaging evidence for language processing.
In contrast to previous simulations\cite{van_der_Velde_2011,van_der_Velde_2010,Frank_2014,van_Dijk_2015}, employing population density techniques implemented in the MIIND software\cite{de_Kamps_2008} allowed us to approximate well transient temporal dynamics.
Thanks to this, we could show an important difference in the shape of the AdEx and LIF responses, how before arriving to a steady state AdEx responds differently to additional input due to adaptation and how the coordination of input and control events is importantly influenced by the model and its parameters.
This last point is crucial for language processing, since ideally~we would want input and control events to be as independent as possible to diminish operational complexity and to be able to operate under a wide range of parameters allowing for random variation.
Reflecting on the coordinating complications introduced by adaptation we would expect lower adaptation in language related brain regions that seem to be involved in binding operations like pSTS, IFGTri and IFGorb, an increase in the operational complexity of the circuits and/or a reduction in the operational parameter ranges alongside additional sensitivity to noise.
A final remark related to event coordination problems in AdEx would be its sensitivity to the synaptic efficacy parameter of the circuit that was not explored in detail.
Contrary to the LIF model, AdEx requires clear limitations in the populations connectivity to implement arbitrary or complex timing operations.


Maybe comment on how averaging neural activity washes out some details about the neural model, even though they themselves were clearly different at the level of one binding.



ECOG

We make a qualitative comparison, given that high gamma power has been shown to be weakly correlated to the firing rate time series of spiking neurons from in-vivo recordings\cite{Ray_2011}. \emph{should go to discussion, now i am just describing the simulation time series}


DISCUSSION ON WHY WE HAVE PROBLEMS WITH BINDING DROP AND MA DROP. MA activity is a lot lower or immediatly release with binding. Two possible interpretations of the sudden activity drop...

DISCUSSION ABOUT MIDDLE SENTENCE FLUCTUATION? This middle fluctuation could be understood as counter evidence to bottom up parsing or an early release of MA activity, that has to be later compensated with reactivation to bind the remaining phrase "of Bill Gates".

DISCUSSION ON LONG ECOG MIDDLE SENTENCE ACTIVITY RELEASE. To accommodate this behavior into the NBA, without changing our grammar and parsing assumptions, we would need to incorporate a limited capacity to sustain MA activity that has to be renovated or a greedy binding mechanism that leads to premature binding mistakes....

DISCUSSION ON HOW TO TUNE LIF AND ADEX TO ECOG DATA
In the case of the LIF simulation, there is flexibility to extend the activity peak with the duration of control events and to increase the total drop magnitude by manipulating the input rates and excitatory connections.
On the other hand the AdEx model loses this flexibility due to adaptation but better emphasizes the contrast between the activity peak and sudden drop because of the pronounced initial activity burst of its population dynamics.



HEMODYNAMICS

\emph{GO TO DISCUSSION. This is a very naive approximation taken for illustrative purposes, that has to be interpreted with caution since the relationship between neural activity, cerebral blood flow and blood oxygenation is not linear\cite{Friston_2000,Buxton_2004} and better represented by the balloon model than the gamma function considered here\cite{Waldorp_2009}.}

DISCUSSION ABOUT THE PEAK ONSET OF THE SIMULATION, TAKING INTO ACCOUNT ECOG DATA. This is unlikely from our previous observations in ECoG recordings, that suggest a much faster decrease of activity after stimuli presentation.
Still this prediction gives a cautionary tale on the extent to which lack of neural modelling can lead to a non trivial misspecification of the events introduced in the GLM model to analyze Bold-fMRI experiments.
Thinking the other way around, the peak onset link to the MA and WM inactivation periods suggest that we could also use Bold-fMRI to fit the parameters of an inactivation mechanism in the NBA.
To realize this, such mechanism would need to be proposed and a more realistic mapping from neural activity to blood oxygenation would need to be implemented.

DISCUSSION. which already seem to be too long with respect to observations made on the intracortical data, as commented in the previous results section \ref{sec:ecog-patterns}.

DISCUSSION. Explore the hypothesis space of mechanisms to stop MA activity after binding. What would signal this? The NBA do not provide a particular proposal in this direction.

DISCUSSION. DISTINGUISH ADEX from LIF
For example that an AdEx model would offer more flexibility in our circuit to manipulate HRF peak amplitudes while minimizing their onsets.




DISCUSSION. Notice that the contrast of the amplitude of word lists with the other conditions can give insights into the relative proportion between MA and WM activity.
As we saw in the ECoG time series, MA seems to have a more important participation in the total neural activity than we initially modeled.
This could explain the~flatter slope of the experimental results.
To reproduce it with our simulations, keeping their qualitative behavior intact, we could select a parameter regime with less excitatory connections and higher input rate.
Note that in the case of the AdEx model, due to the shape of the adaptive response, it would be possible to reproduce the sublinear peak amplitude effect while minimizing a peak onset difference.
To achieve this we could make all responses approximate the same center of mass by mixing the high initial increase of firing rate, that depend mostly on the input rate, with a very low steady state rate.
Nonetheless, to make a quantitative fit to the hemodynamic amplitudes we should use a more realistic hemodynamic model and be cautious about the possible model misspecifications introduced by the simultaneous consideration of the HRF peak onsets.


DISCUSSION. The super-linear increase of peak onset is not coherent with sub-linear patterns reported by Pallier et al.
Nonetheless the peak onset of our simulation depends on the input events and Working Memory durations, that were arbitrarily set to a constant duration.
The Neural Blackboard Architecture does not provide a particular hypothesis on the timing of the deactivation of Main-Assemblies and Working Memory, which is why durations were simply set to a pragmatic constant that secured binding of the last phrasal node with the first word of the longest phrase.
Comparison of our simulation with intracortical recordings in results section \ref{sec:ecog-patterns} suggested a quicker drop of the Main-Assemblies activity after binding operations were executed, instead of the current choice of persistent activity for a constant amount of time after binding.
Modifying the simulation mechanisms in this direction would permit emulating sub-linear patterns of peak onset as necessary to fit the hemodynamic measurements.


DISCUSSION. Pallier et al. initially hypothesized a linear pattern instead of the sub-linear one, which would be explained by a simple "accumulation" model where each new word presented would add a constant amount of neural activity until a binding was not possible, leading to a sudden drop of activity back to baseline.
After their finding, the authors revised their hypothesis to propose instead a model that assigns a logarithmic increase of activity to each new word presented.
Nonetheless our simulation suggest another explanation for the sub-linear pattern as a direct reflection of the number of binding operations executed while processing phrases.
It turns out that the type of stimuli employed by the authors consisted exclusively of right branching trees and that their concatenation lead to a sub-linear increase of number of binding operations, which is why our simulation is at a first sight coherent with the logarithmic word activity addition model.
Our simulation suggest then, that assigning a logarithmic increase of activity to the next word presented in a phrase is an artifact of the experimental design to be challenged by another design including other syntactic tree shapes, for which our model would predict the responses as a function of the number of bindings.




\subsection{Future perspective}



We can apply the complete phrase neural time series approximation to a variety of stimuli from experimental designs behind neuroimaging results in the literature. \emph{for discussion, not done, only two experiments}

This could be done for example by using the simulation as an experiment design tool that provides the set of phrases that maximize the spatio-temporal differences in neural responses across conditions defined under the different grammar theories and parsing schemes.

Propose what to do next. from cytoarchitecture to better physical models to iteration between ecog hypothesis experimental design and simulation effects. becomes a tool for the linguist.


WE COULD ADAPT THE SIMULATION PARAMETERS TO BETTER REFLECT THE ECOG TIME SERIES
It could further be tuned to fit quantitatively the data by manipulating the number of excitatory connections, such that their decrease would reduce the baseline difference.
The irregularities of the magnitude of the baseline effect with respect to phrase length in the high gamma power is difficult to interpret, since it could suggest that the accumulation of activity with more recruiting nodes is subtler than what we can reproduce with the NBA circuit, in which there is a clear correspondence with the number of hypothesized compartment circuits of the parsed tree structure.
Moreover, when comparing simulations, we appreciate that the AdEx model could better reflect a more modest proportional increase of baseline thanks to adaptation.~


Finally, even though there are still many limitations in these simulations, we would like to emphasize the quick progress in the development of biologically plausible models of cognition.
With an additional effort it would be possible to fit parameters of the circuit and phrase processing directly to neuroimaging measurements.
Also diverse new computational methods like population density techniques have made it feasible to approximate at a circuit scale point neural models as complex as the adaptive exponential.
There are as well successful recent efforts in modeling, with cytoarchitectonic details, complex signals like Local Field Potentials~\cite{Mazzoni_2015,Hagen_2015} and hemodynamics with the balloon model~\cite{Buxton_2004}.
These could be already adapted to our circuit simulations to better incorporate neuroimaging evidence simultaneously from multiple techniques and experimental designs.
Moreover the circuit itself can become a tool for hypothesis exploration and experimental design on the different neuroimaging techniques.


\noteMdK{Is the below relevant?} 
\notenewMP{Now that I read it I agree presented like this is incomplete. I think it would be a very important future extension of the blackboard. Replacing a neural population by its cytoarchitectonic equivalent to be able to generate LFP predictions and also to predict spatial extent of the Blackboard. Right now for example is still ambiguous for me if the circuit can be implemented in more or less than 1 cm of cortex, and that have important implications to also interpret Bold-fMRI activations. Since PDTs approximate distributions of very large populations, also I am not sure how the populations would behave under cytoarchitectonic constraints. Maybe what I just wrote here would be a nice addition to the discussion with the paragraph below instead of being here?} 
It could also be important to consider parallely multiple neuron typologies and modelling their corresponding microcircuitry of cortical columns, as attempted already with LIF models, to simulate local field potentials for a very small patch of cortex\cite{Mazzoni_2015,Hagen_2015}.\\~\\


Add to final discussion how the model can be turned into a strong inference testing tool with some additional modifications. Cytoarchitecture - LFP - ECoG.


In conclusion we hope~to have shown that we are close to producing biologically realistic mechanistic neural models of cognitive function, that could provide new ways of testing cognitive hypothesis on varied neuroimaging techniques, to further~inspire more work in this direction.


\selectlanguage{english}

\bibliography{bibliography/converted_to_latex}

\end{document}

